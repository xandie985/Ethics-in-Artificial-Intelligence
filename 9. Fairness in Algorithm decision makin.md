## Fairness in ADM (Algo Decision Making)
Era of Big Data - automated decision making in multiple domains.
Debate of prospects and risks of Algo assessment of people/group for making decisions.

One side - 
* ADM are cheaper, precise and imapartial.
* ADM avoid issues related to human psychology. (overconfidence/ loss aversion/ anchoring/etc.)
* In many fields machines have outperormed human experts. 

Another side -
* ADM can be discriminatory
* ADM can make decisions on prohibited features (race/gender/color/etc.)

Discrimination causes?
1. Humar Error Propagation - systerm based on supervised learning trained on past human judgements - produces similar decisions. Similar errors and predujice will be carried forward.
2. Trainig Set composition - The biases introduced in the training set can cause systems to make discrimination among individuals. Blacks had more chace of getting longer sentences based on similar offence as compared to whites.
3. Feature Selection - the kind of features we use for particular target. Only relevant features should be used. chances of system making decisons baised on ethinicity/social group.

Unfairness in dataset can come from underrepresented group of people. 
Difficult to challenge the unfairness of automated DM. Decision made my algo can be tough to reason upona and expenesive on individual cases. Using stastical data to justify the decision can be proved irrelvant.
Algorithms can be the weapons on future. 
Algorithms are more controllable as compared to human decision makers. 
They can be improved/re-engineered.
Issue of AI explainability. 

Should we exclude the use of automated DM?
No, because the system itself is imperfect, and ADM can improve it.
Need of hybrid approach, where there are human reviewers in case of sensitive/important decisions.  
There is need to work on transparency of system's explainability. 
The ADM should be flexible to be tweaked in cases, where irrational decision are made by system. 

**Principle of fairness:** no bias, discrimination, stigmatisation. 
GDRP says - The individuals should be informed about automated sys, its purpose, profiling & possible consequences.
How to avoid prejudice and discrimination:
1. using appropriate mathemaitical/statisticall procedures for profiling.
2. implementing technical/organisation measure to ensure corectness of data.
3. Securing personal data from risks & discriminationatory effects.

### COMPAS system
A risk assessment tool used in the American court to predict the recidivism.
Based on alogrithm that compares people to grp of indivdual based on characterstics. 
Scores are based on MCQ. 
* Loomis Case 
- stole vehicle and fled police. COMAPS - 6 years jailtime. 
- Decision was apealed by Loomis, and arguments were rejected byb Supreme Court by following statemnets:
1. It is used to enhance judge's evaluation as COMPAS is based on stats and historic data. 
2. No gender discrimination was considered as different sex have different rate of recedivism.
3. COMAPAS gives high risk to blacks so judge should be carefult.

ProRepublica - independent study:
High Risk misclassifcation - 54% Black and 23% Whites.
Low Risk misclassifcation - 48% W and 28% B.

Is COMPAS fair and Accurate?
SAPMOC - hypothetical system similar to COMPAS. We study it's performance.
* Two groups are equally represented in training set.
* System takes into account whether or not the defendant committed previous criminal act. 
* Outcome- criminals with historical background were asgined high risk as compared tono.short history of offense.
* The accuracy for both group is same.
* *Stastical Parity* - each grp should have equal proportion of +ve and -ve offense. Balanced Training set.
* *Equality of Opportunity* - This is due to differce in base case where blues have done 75% of crimes and Greens have donoe 25% of crimes. Hences the system punishes Blue more.
![image](https://user-images.githubusercontent.com/18325219/172943330-8eaf9cf0-1f9e-45af-9278-97aef22be580.png) 
* *Calibration* -predcitions should be correct within each group. ![image](https://user-images.githubusercontent.com/18325219/172943869-f2a00e87-7bb3-4b55-92bb-77c1bddb5e37.png)
* *Condiational User Error (False Rate)* - FP & FN are considered here ![image](https://user-images.githubusercontent.com/18325219/172944101-cebe0124-e217-4fa6-878b-cc3829d7e7cb.png)
* *Treatment Equality* - ratio between errors between all groups should be eequal. ![image](https://user-images.githubusercontent.com/18325219/172944290-176ec929-b04b-45b7-8a43-4526367d3a71.png)




